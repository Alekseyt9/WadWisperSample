<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>VAD Example</title>
	<script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.js"></script>
	<script src="https://cdn.jsdelivr.net/npm/@ricky0123/vad-web@0.0.7/dist/bundle.min.js"></script>	
</head>
<body>
    <h1>Whisper API realtime example</h1> 
	
	<label for="apiKey">API Key:</label>
    <input type="text" id="apiKey" name="apiKey">
	<br><br>
	
	<label for="positiveThreshold">Positive Speech Threshold:</label>
    <input type="number" id="positiveThreshold" name="positiveThreshold" min="0" max="1" step="0.01" value="0.5"><br><br>

    <label for="negativeThreshold">Negative Speech Threshold:</label>
    <input type="number" id="negativeThreshold" name="negativeThreshold" min="0" max="1" step="0.01" value="0.35"><br><br>
	
	<label for="minSpeechFrames">min Speech Frames:</label>
    <input type="number" id="minSpeechFrames" name="minSpeechFrames" min="1" max="1000" step="1" value="3"><br><br>

	<label for="preSpeechPadFrames">pre Speech Pad Frames:</label>
    <input type="number" id="preSpeechPadFrames" name="preSpeechPadFrames" min="1" max="1000" step="1" value="1"><br><br>

	<label for="redemptionFrames">redemption Frames:</label>
    <input type="number" id="redemptionFrames" name="redemptionFrames" min="1" max="1000" step="1" value="8"><br><br>

	<button id="startButton">Update</button>
	<br><br>

	<textarea id="transcriptionOutput" rows="20" cols="100"></textarea>
	
	<script>
	  let queue = [];
	  let isProcessing = false;
	  let myvad = null;
	  
	  document.getElementById('startButton').addEventListener('click', function() {
            main(); 
        });

	  async function processQueue() {
		if (isProcessing) return;
		isProcessing = true;

		while (queue.length > 0) {
		  const file = queue.shift();
		  try {
			const response = await callWhisperApi2(file);
			console.log('Whisper API response:', response);
			document.getElementById('transcriptionOutput').value += response.text + '\n';
		  } catch (error) {
			console.error('Error in callWhisperApi:', error);
		  }
		}

		isProcessing = false;
	  }
	
	  async function main() {
	  
	    let positiveThreshold = document.getElementById('positiveThreshold');
		let negativeThreshold = document.getElementById('negativeThreshold');
		let minSpeechFrames = document.getElementById('minSpeechFrames');
		let preSpeechPadFrames = document.getElementById('preSpeechPadFrames');
		let redemptionFrames = document.getElementById('redemptionFrames');
			
	    const positiveThresholdValue = parseFloat(positiveThreshold.value);
        const negativeThresholdValue = parseFloat(negativeThreshold.value);
        const minSpeechFramesValue = parseInt(minSpeechFrames.value);
		const preSpeechPadFramesValue = parseInt(preSpeechPadFrames.value);
		const redemptionFramesValue = parseInt(redemptionFrames.value);
	  
		if (myvad) {
          myvad.pause();
        }
	  
		myvad = await vad.MicVAD.new({
			positiveSpeechThreshold: positiveThresholdValue,
            negativeSpeechThreshold: negativeThresholdValue,
			minSpeechFrames: minSpeechFramesValue,
			preSpeechPadFrames: preSpeechPadFramesValue,
			redemptionFrames: redemptionFramesValue, 
		  onSpeechEnd: async (audio) => {
	
			const wavBuffer = vad.utils.encodeWAV(audio)
			var file = new File([wavBuffer], `file${Date.now()}.wav`)

			queue.push(file); 
            processQueue(); 
				
		  }
		})
		myvad.start()
		
		/*
		let valChanged = function() {
			myvad.options.positiveThreshold = parseFloat(positiveThreshold.value);
			myvad.options.negativeThreshold = parseFloat(negativeThreshold.value);
			myvad.options.minSpeechFrames = parseFloat(minSpeechFrames.value);
			myvad.options.preSpeechPadFrames = parseFloat(preSpeechPadFrames.value);
			myvad.options.redemptionFrames = parseFloat(redemptionFrames.value);
		};
		
		positiveThreshold.addEventListener('input', valChanged);
		negativeThreshold.addEventListener('input', valChanged);
		minSpeechFrames.addEventListener('input', valChanged);
		preSpeechPadFrames.addEventListener('input', valChanged);
		redemptionFrames.addEventListener('input', valChanged);
		*/
	  }
	  
	  async function callWhisperApi2(audioFile) {
	  
		const apiKey = document.getElementById('apiKey').value; 
		if (!apiKey) {
			console.error('API Key is missing!');
			return;
		}
	  
		const formData = new FormData();
		formData.append('file', audioFile);
		formData.append('model', 'whisper-1'); 
		formData.append('language', 'ru'); 
		
		try {
			const response = await fetch('https://api.openai.com/v1/audio/transcriptions', {
				method: 'POST',
				headers: {
					'Authorization': 'Bearer '+apiKey,
				},
				body: formData
			});

			if (!response.ok) {
				throw new Error(`HTTP error! status: ${response.status}`);
			}

			const data = await response.json();
			return data;
		} catch (error) {
			console.error('Error calling Whisper API:', error);
		}
	  }

	  main()
	</script>
</body>
</html>
